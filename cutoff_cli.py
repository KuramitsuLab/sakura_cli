#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Loss Calculation CLI Tool
æŒ‡å®šã•ã‚ŒãŸå¹´ã®å„æœˆã®æœ€åˆã®æ—¥ã®æ–‡å­—åˆ—ã«å¯¾ã—ã¦LLMã®losså€¤ã‚’è¨ˆç®—ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import argparse
import csv
import datetime
import os
import sys
from typing import List, Tuple

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm


def get_weekday_name(date: datetime.date, lang: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®æ›œæ—¥åã‚’æŒ‡å®šã•ã‚ŒãŸè¨€èªã§å–å¾—"""
    weekdays = {
        'en_us': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],
        'ja': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'],
        'zh': ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥'],
        'kr': ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
    }
    
    return weekdays[lang][date.weekday()]


def generate_date_string(year: int, month: int, lang: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸå¹´æœˆã®æœ€åˆã®æ—¥ã®æ–‡å­—åˆ—ã‚’ç”Ÿæˆ"""
    date = datetime.date(year, month, 1)
    weekday = get_weekday_name(date, lang)
    
    if lang == 'en_us':
        return f"Event Date: {date.strftime('%B %d, %Y')} ({weekday})"
    elif lang == 'ja':
        return f"é–‹å‚¬æ—¥: {year}å¹´{month}æœˆ{date.day}æ—¥ï¼ˆ{weekday}ï¼‰"
    elif lang == 'zh':
        return f"ä¸¾åŠæ—¥æœŸ: {year}å¹´{month}æœˆ{date.day}æ—¥ï¼ˆå‘¨{weekday}ï¼‰"
    elif lang == 'kr':
        return f"ê°œìµœì¼: {year}ë…„ {month}ì›” {date.day}ì¼ ({weekday}ìš”ì¼)"
    else:
        raise ValueError(f"Unsupported language: {lang}")


def calculate_loss(model, tokenizer, text: str, device: str) -> float:
    """æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®losså€¤ã‚’è¨ˆç®—"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss.item()
    
    return loss


def parse_year_range(year_str: str) -> List[int]:
    """å¹´ã®ç¯„å›²æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦å¹´ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if '-' in year_str:
        # ç¯„å›²æŒ‡å®š (ä¾‹: "2019-2022")
        start_year, end_year = map(int, year_str.split('-'))
        if start_year > end_year:
            raise ValueError(f"Invalid year range: {year_str}. Start year must be <= end year")
        return list(range(start_year, end_year + 1))
    else:
        # å˜ä¸€å¹´ (ä¾‹: "2022")
        return [int(year_str)]


def load_model_and_tokenizer(model_path: str, device: str) -> Tuple:
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰"""
    print(f"Loading model from: {model_path}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        
        # ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®š
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model.to(device)
        model.eval()
        
        print(f"Model loaded successfully on {device}")
        return model, tokenizer
        
    except Exception as e:
        print(f"Error loading model: {e}")
        sys.exit(1)


def detect_loss_spikes(results: List[dict], min_diff: float = 0.5) -> List[dict]:
    """å‰3ãƒ¶æœˆã®å¹³å‡å€¤ã«å¯¾ã—ã¦æœ€ã‚‚å¤§ããªå·®åˆ†ã‚’ç¤ºã—ãŸæœˆã‚’æ¤œå‡ºã™ã‚‹"""
    if len(results) < 4:  # æœ€ä½4ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå¿…è¦ï¼ˆ3ãƒ¶æœˆ + ç¾åœ¨æœˆï¼‰
        return []
    
    spikes = []
    
    for i in range(3, len(results)):  # å‰3ãƒ¶æœˆã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
        # å‰3ãƒ¶æœˆã®losså€¤ã®å¹³å‡ã‚’è¨ˆç®—
        prev_3_months = [results[j]['loss'] for j in range(i-3, i)]
        mean_prev_3 = sum(prev_3_months) / len(prev_3_months)
        
        current_loss = results[i]['loss']
        
        # å·®åˆ†ã‚’è¨ˆç®—
        diff = current_loss - mean_prev_3
        
        # æœ€å°å·®åˆ†é–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®ã¿ã‚¹ãƒ‘ã‚¤ã‚¯ã¨ã—ã¦è¨˜éŒ²
        if diff > min_diff:
            spike_info = {
                'year': results[i]['year'],
                'month': results[i]['month'],
                'current_loss': current_loss,
                'prev_3_mean': mean_prev_3,
                'diff': diff,
                'prev_3_months': prev_3_months,
                'month_index': i
            }
            spikes.append(spike_info)
    
    return spikes


def save_to_csv(results: List[dict], save_file: str):
    """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ä¿å­˜"""
    fieldnames = ['model_path', 'lang', 'year', 'month', 'loss']
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    file_exists = os.path.exists(save_file)
    
    with open(save_file, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿
        if not file_exists:
            writer.writeheader()
        
        writer.writerows(results)
    
    if file_exists:
        print(f"Results appended to: {save_file}")
    else:
        print(f"New file created: {save_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Calculate LLM loss values for monthly date strings",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  python3 cutoff_cli.py --model_path gpt2 --year 2022 --lang ja --save_file results.csv
  python3 cutoff_cli.py --model_path gpt2 --year 2019-2022 --lang ja --save_file results.csv
  python3 cutoff_cli.py --model_path microsoft/DialoGPT-medium --year 2023 --lang en_us --spike_threshold 1.0
        """
    )
    
    parser.add_argument(
        '--model_path',
        required=True,
        help='Hugging Face model path or local model directory'
    )
    
    parser.add_argument(
        '--year',
        required=True,
        help='Target year or year range (e.g., "2022" or "2019-2022")'
    )
    
    parser.add_argument(
        '--lang',
        choices=['en_us', 'ja', 'zh', 'kr'],
        required=True,
        help='Language for date string generation'
    )
    
    parser.add_argument(
        '--save_file',
        default='loss_results.csv',
        help='CSV file path to save results (default: loss_results.csv)'
    )
    
    parser.add_argument(
        '--device',
        choices=['cpu', 'cuda'],
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help='Device to run the model on'
    )
    
    parser.add_argument(
        '--spike_threshold',
        type=float,
        default=0.5,
        help='Minimum difference from 3-month average to detect as spike (default: 0.5)'
    )
    
    args = parser.parse_args()
    
    # å¹´ã®ç¯„å›²ã‚’ãƒ‘ãƒ¼ã‚¹
    try:
        years = parse_year_range(args.year)
    except ValueError as e:
        print(f"Error: {e}")
        sys.exit(1)
    
    # å…¥åŠ›æ¤œè¨¼
    for year in years:
        if year < 1900 or year > 2100:
            print(f"Error: Year {year} must be between 1900 and 2100")
            sys.exit(1)
    
    # ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("Warning: CUDA not available, using CPU")
        args.device = 'cpu'
    
    print(f"Configuration:")
    print(f"  Model: {args.model_path}")
    print(f"  Years: {years[0] if len(years) == 1 else f'{years[0]}-{years[-1]}'} ({len(years)} years)")
    print(f"  Language: {args.lang}")
    print(f"  Device: {args.device}")
    print(f"  Save file: {args.save_file}")
    print(f"  Spike detection threshold: +{args.spike_threshold} from 3-month average")
    print()
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model_and_tokenizer(args.model_path, args.device)
    
    # å…¨ä½“ã®é€²æ—ã‚’è¨ˆç®—
    total_months = len(years) * 12
    results = []
    
    # tqdmã§ã®é€²æ—è¡¨ç¤º
    with tqdm(total=total_months, desc="Processing", unit="month") as pbar:
        for year in years:
            for month in range(1, 13):  # 1æœˆã‹ã‚‰12æœˆã¾ã§
                try:
                    # æ—¥ä»˜æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                    date_string = generate_date_string(year, month, args.lang)
                    
                    # losså€¤ã‚’è¨ˆç®—
                    loss = calculate_loss(model, tokenizer, date_string, args.device)
                    
                    # çµæœã‚’ä¿å­˜
                    result = {
                        'model_path': args.model_path,
                        'lang': args.lang,
                        'year': year,
                        'month': month,
                        'loss': round(loss, 6)
                    }
                    results.append(result)
                    
                    # é€²æ—ãƒãƒ¼ã®èª¬æ˜ã‚’æ›´æ–°
                    pbar.set_postfix({
                        'Year': year,
                        'Month': f'{month:02d}',
                        'Loss': f'{loss:.4f}'
                    })
                    pbar.update(1)
                    
                except Exception as e:
                    print(f"\nError processing {year}/{month:02d}: {e}")
                    pbar.update(1)
                    continue
    
    # çµæœã‚’CSVã«ä¿å­˜
    if results:
        save_to_csv(results, args.save_file)
        print(f"\nCompleted! Processed {len(results)}/{total_months} months successfully.")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        if len(results) > 0:
            losses = [r['loss'] for r in results]
            print(f"\nLoss statistics:")
            print(f"  Min: {min(losses):.6f}")
            print(f"  Max: {max(losses):.6f}")
            print(f"  Mean: {sum(losses)/len(losses):.6f}")
            
            # å‰3ãƒ¶æœˆå¹³å‡ã‹ã‚‰ã®å·®åˆ†ã§lossã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º
            spikes = detect_loss_spikes(results, args.spike_threshold)
            if spikes:
                print(f"\nâš ï¸  Loss spikes detected ({len(spikes)} occurrences):")
                print("   Year/Month    Current    3-Month Avg    Difference    Previous 3 Months")
                print("   " + "="*85)
                for spike in spikes:
                    prev_months_str = " ".join([f"{x:.3f}" for x in spike['prev_3_months']])
                    print(f"   {spike['year']}/{spike['month']:02d}        "
                          f"{spike['current_loss']:.4f}      "
                          f"{spike['prev_3_mean']:.4f}       "
                          f"+{spike['diff']:.4f}       "
                          f"[{prev_months_str}]")
                
                # æœ€ã‚‚å¤§ããªå·®åˆ†ã‚’ç‰¹åˆ¥ã«è¡¨ç¤º
                max_spike = max(spikes, key=lambda x: x['diff'])
                print(f"\nğŸ“ˆ Largest spike: {max_spike['year']}/{max_spike['month']:02d} "
                      f"(+{max_spike['diff']:.4f} above 3-month average)")
                
                # è¿½åŠ ã®åˆ†ææƒ…å ±
                print(f"\nğŸ“Š Spike analysis:")
                differences = [s['diff'] for s in spikes]
                print(f"   Average difference: +{sum(differences)/len(differences):.4f}")
                print(f"   Range: +{min(differences):.4f} to +{max(differences):.4f}")
                
                # æœ€åˆã®ã‚¹ãƒ‘ã‚¤ã‚¯ã®æ™‚æœŸã‚’ç‰¹å®šï¼ˆçŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•ã®å¯èƒ½æ€§ï¼‰
                first_spike = min(spikes, key=lambda x: (x['year'], x['month']))
                print(f"   First spike detected: {first_spike['year']}/{first_spike['month']:02d} "
                      f"(potential knowledge cutoff)")
            else:
                print(f"\nâœ… No significant spikes detected (threshold: +{args.spike_threshold} from 3-month average)")
    else:
        print("No results to save.")
        sys.exit(1)


if __name__ == "__main__":
    main()

